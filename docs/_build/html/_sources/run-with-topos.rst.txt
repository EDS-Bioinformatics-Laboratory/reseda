Run with ToPoS
==============

How to run - Using ToPoS for sending jobs and using a job monitor tool

Note: this is what Barbera does for each sequence run

ToPoS: https://topos.grid.sara.nl/4.1/

Job monitoring tool: https://bitbucket.org/barbera/progress/

Helper scripts
--------------

Fill in the run name and output directory in ENV.sh and source the script. You
can use the environment variables when you go through the steps below.

.. code-block:: bash

   RUN=run30-20180723-miseq
   OUTDIR=results-tbcell
   WEBDAV=https://researchdrive.surfsara.nl/remote.php/webdav/amc-immunogenomics/RUNS/${RUN}
   POOLNAME=d8c24f78f9772cbdff54cf62

Files can be copied from the webdav server by adding file paths in SAMPLES and
then use:

.. code-block:: bash

   ./copy-from-beehub.sh

Example of the content in the SAMPLES file:

.. code-block:: bash

   /mnt/immunogenomics/RUNS/TESTDATA/data/test-B_S1_L001_R1_001.fastq.gz
   /mnt/immunogenomics/RUNS/TESTDATA/data/test-B_S1_L001_R2_001.fastq.gz
   /mnt/immunogenomics/RUNS/TESTDATA/data/test-T_S2_L001_R1_001.fastq.gz
   /mnt/immunogenomics/RUNS/TESTDATA/data/test-T_S2_L001_R2_001.fastq.gz

Data can be transferred from the webdav server with:

.. code-block:: bash

   ./copy-to-webdav.sh WEBDAVURL FILE1 [FILE2]

Preparation
-----------

Mount basespace. Instructions are in basespace.txt

Specify the run and the basespace sub-directories as argument to
copy-basespace-data-to-beehub.py and run it. The file basespace-copy-data.sh
and basespace-calc-checksum.sh will be created. Run these scripts to copy the
data from basespace to the ResearchDrive and to calculate the SHA1 sums
(last one is needed for the VerifyBasespaceCopy.py script)

.. code-block:: bash

   python copy-basespace-data-to-beehub.py -r runNN-yyyymmdd-miseq run-dir-in-project-dir-on-basespace
   nohup bash basespace-copy-data.sh > nohup-copy.out 2> nohup-copy.err < /dev/null &
   nohup bash basespace-calc-checksum.sh > nohup-check.out 2> nohup-check.err < /dev/null &

Convert the MiSeq sample sheet with MetaData.py (creates a json file)

.. code-block:: bash

   python MetaData.py Miseq-sample-Datasheet.csv

Mount the ResearchDrive webdav server

.. code-block:: bash

   sudo mount -t davfs -o uid=bioinfo,gid=bioinfo,rw https://researchdrive.surfsara.nl/remote.php/webdav/amc-immunogenomics /mnt/immunogenomics

Verify if all data has been copied

.. code-block:: bash

   python2 VerifyBasespaceCopy.py |grep grep

You need the mounted ResearchDrive directory name and the json file from the previous step.

Add extra information to the json file with MakeSamplesFiles.py (this will also make the SAMPLE-* files)

.. code-block:: bash

   python MakeSamplesFiles.py -r yyyymmdd-RUNnn-datasheet.json -w /mnt/immunogenomics/RUNS/runNN-yyyymmdd-miseq/data/

Sort and split the SAMPLE-* files with: SortAndSplit.sh It does the following:

    * Sorts the SAMPLE-* files: sort SAMPLE-blah > SAMPLE-blah.sort
    * Makes manageable jobs by splitting the SAMPLE-\*.sort files, e.g.: split -l 20 SAMPLES-run13-human-BCRh.sort SAMPLES-run13-human-BCRh-

.. code-block:: bash

   ./SortAndSplit.sh SAMPLE-*

Create Topos jobs with ToposCreateTokens.py (run with the -h option to see the arguments)

.. code-block:: bash

   python ToposCreateTokens.py -r runNN-yyyymmdd-miseq -m MIDS-miseq-umi.txt -o results-tbcell -p paired -b yes -u yes

Upload Topos jobs with ToposUploadFiles.py (run without arguments to see the arguments)

.. code-block:: bash

   python ToposUploadFiles.py d8c24f78f9772cbdff54cf62 tokens/*

Starting the jobs
-----------------

Start virtual machines for the analysis (in the SurfSara HPC cloud webinterface)
https://ui.hpccloud.surfsara.nl/

Add all ip-adresses to the ip-list file in the '../progress' directory (the job monitoring tool)

Transfer the setup-and-run.sh by running roll-out-scripts.py and executing the
code that was generated by this roll-out script

.. code-block:: bash

   python roll-out-scripts.py > tmp.sh
   bash tmp.sh

Login to each machine and run:

.. code-block:: bash

   ./setup-and-run.sh

When all jobs are finished
--------------------------

In the jobs the data is automatically transferred to the ResearchDrive webdav server

Execute ConcatenateCloneFilesBatch.py to generate a bash script for
concatenating clone files per project+organism+cell_type (run the generated script)

.. code-block:: bash

   python ConcatenateCloneFilesBatch.py -r yyyymmdd-RUNnn-datasheet-new.json -w /mnt/immunogenomics/RUNS/runNN-yyyymmdd-miseq/results-tbcell/final/ > tmp.sh
   nohup bash tmp.sh > nohup-concat.out 2> nohup-concat.err < /dev/null &


Run report-ALL.sh to generate reports about the sequence run (help is available for this script if you do not give arguments)

.. code-block:: bash

   ./report-ALL.sh -r runNN-YYYYMMDD-miseq -i yyyymmdd_RUNnn_Datasheet-new.json -b yes -o results-tbcell

Check for contamination with contamination-figure.R or the pandas-sample-similarity.ipynb notebook

* Specify the files that were created by ConcatenateCloneFilesBatch.py
* Specify the pt.table.csv that you got from the immunogenomics group
* Check by hand if the column names in the pt.table are correct
* Run the script
* Usually the reports are made for all samples per cell_type
